[{"authors":["admin"],"categories":null,"content":"I am an Audio Machine Learning Scientist at Bose Corporation. My current research is centered around the development of novel ML-based methods for lightweight, on-device speech and audio signal processing, with a particular focus on speech enhancement and hearing augmentation.\nBefore, I was a PhD student in the Department of Bioengineering \u0026amp; Centre for Neurotechnology at Imperial College London (ICL). As a member of the Sensory Neuroengineering lab led by Prof. Tobias Reichenbach, my research focused on understanding neural mechanisms underlying perception and comprehension of natural speech, especially in challenging listening conditions. In my work, I combined computational modelling with neuroimaging and non-invasive brain stimulation to understand how natural speech is processed across human auditory pathways.\nIn addition to my PhD research, I also worked as an Applied Scientist Intern at Amazon - Lab 126, a Scientific Advisor at Logitech and a Consultant for clinical data analysis at INBRAIN Neuroelectronics.\nFor more details, see my CV, explore this website or get in touch!\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://mkegler.github.io/author/mikolaj-kegler-ph.d./","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/mikolaj-kegler-ph.d./","section":"authors","summary":"I am an Audio Machine Learning Scientist at Bose Corporation. My current research is centered around the development of novel ML-based methods for lightweight, on-device speech and audio signal processing, with a particular focus on speech enhancement and hearing augmentation.","tags":null,"title":"Mikolaj Kegler, Ph.D.","type":"authors"},{"authors":["Rayan Daod Nathoo*","Mikolaj Kegler*","Marko Stamenovic"],"categories":null,"content":"","date":1693785600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693785600,"objectID":"4e0c9dcd25a5e5476eeba50a7fd46415","permalink":"https://mkegler.github.io/publication/daod-nathoo-2023/","publishdate":"2023-09-04T00:00:00Z","relpermalink":"/publication/daod-nathoo-2023/","section":"publication","summary":"Tiny, causal models are crucial for embedded audio machine learning applications. Model compression can be achieved via distilling knowledge from a large teacher into a smaller student model. In this work, we propose a novel two-step approach for tiny speech enhancement model distillation. In contrast to the standard approach of a weighted mixture of distillation and supervised losses, we firstly pre-train the student using only the knowledge distillation (KD) objective, after which we switch to a fully supervised training regime. We also propose a novel fine-grained similarity-preserving KD loss, which aims to match the student's intra-activation Gram matrices to that of the teacher. Our method demonstrates broad improvements, but particularly shines in adverse conditions including high compression and low signal to noise ratios (SNR), yielding signal to distortion ratio gains of 0.9 dB and 1.1 dB, respectively, at -5 dB input SNR and 63x compression compared to baseline.","tags":null,"title":"Two-Step Knowledge Distillation for Tiny Speech Enhancement\"","type":"publication"},{"authors":["Bryce Irvin","Marko Stamenovic","Mikolaj Kegler","Li-Chia Yang"],"categories":null,"content":"","date":1685836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685836800,"objectID":"97a73d56d17f9aea0b13a6495bb06ab5","permalink":"https://mkegler.github.io/publication/irvin-2022/","publishdate":"2023-06-04T00:00:00Z","relpermalink":"/publication/irvin-2022/","section":"publication","summary":"Modern speech enhancement (SE) networks typically implement noise suppression through time-frequency masking, latent representation masking, or discriminative signal prediction. In contrast, some recent works explore SE via generative speech synthesis, where the system's output is synthesized by a neural vocoder after an inherently lossy feature-denoising step. In this paper, we propose a denoising vocoder (DeVo) approach, where a vocoder accepts noisy representations and learns to directly synthesize clean speech. We leverage rich representations from self-supervised learning (SSL) speech models to discover relevant features. We conduct a candidate search across 15 potential SSL front-ends and subsequently train our vocoder adversarially with the best SSL configuration. Additionally, we demonstrate a causal version capable of running on streaming audio with 10ms latency and minimal performance degradation. Finally, we conduct both objective evaluations and subjective listening studies to show our system improves objective metrics and outperforms an existing state-of-the-art SE model subjectively.","tags":null,"title":"Self-Supervised Learning for Speech Enhancement through Synthesis","type":"publication"},{"authors":["Gasser Elbanna","Neil Scheidwasser-Clow","Mikolaj Kegler","Pierre Beckmann","Karl El Hajal","Milos Cernak"],"categories":null,"content":"","date":1670976000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670976000,"objectID":"e3bd457e751e4eb9d723d8830edd41fe","permalink":"https://mkegler.github.io/publication/elbanna-2022b/","publishdate":"2022-12-14T00:00:00Z","relpermalink":"/publication/elbanna-2022b/","section":"publication","summary":"Methods for extracting audio and speech features have been studied since pioneering work on spectrum analysis decades ago. Recent efforts are guided by the ambition to develop general-purpose audio representations. For example, deep neural networks can extract optimal embeddings if they are trained on large audio datasets. This work extends existing methods based on self-supervised learning by bootstrapping, proposes various encoder architectures, and explores the effects of using different pre-training datasets. Lastly, we present a novel training framework to come up with a hybrid audio representation, which combines handcrafted and data-driven learned audio features. All the proposed representations were evaluated within the HEAR NeurIPS 2021 challenge for auditory scene classification and timestamp detection tasks. Our results indicate that the hybrid model with a convolutional transformer as the encoder yields superior performance in most HEAR challenge tasks.","tags":null,"title":"BYOL-S: Learning Self-supervised Speech Representations by Bootstrapping","type":"publication"},{"authors":["Gasser Elbanna","Alice Biryukov","Neil Scheidwasser-Clow","Lara Orlandic","Pablo Mainar","Mikolaj Kegler","Pierre Beckmann","Milos Cernak"],"categories":null,"content":"","date":1663545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663545600,"objectID":"0fa25d239f1542d8ee10b2f4f4769e85","permalink":"https://mkegler.github.io/publication/elbanna-2022/","publishdate":"2022-09-19T00:02:40.369174Z","relpermalink":"/publication/elbanna-2022/","section":"publication","summary":"As a neurophysiological response to threat or adverse conditions, stress can affect cognition, emotion and behaviour with potentially detrimental effects on health in the case of sustained exposure. Since the affective content of speech is inherently modulated by an individual's physical and mental state, a substantial body of research has been devoted to the study of paralinguistic correlates of stress-inducing task load. Historically, voice stress analysis (VSA) has been conducted using conventional digital signal processing (DSP) techniques. Despite the development of modern methods based on deep neural networks (DNNs), accurately detecting stress in speech remains difficult due to the wide variety of stressors and considerable variability in the individual stress perception. To that end, we introduce a set of five datasets for task load detection in speech. The voice recordings were collected as either cognitive or physical stress was induced in the cohort of volunteers, with a cumulative number of more than a hundred speakers. We used the datasets to design and evaluate a novel self-supervised audio representation that leverages the effectiveness of handcrafted features (DSP-based) and the complexity of data-driven DNN representations. Notably, the proposed approach outperformed both extensive handcrafted feature sets and novel DNN-based audio representation learning approaches.","tags":null,"title":"Hybrid Handcrafted and Learnable Audio Representation for Analysis of Speech Under Cognitive and Physical Load","type":"publication"},{"authors":["Manuel Cherep","Mikolaj Kegler","Jean-Philippe Thiran","Pablo Mainar"],"categories":null,"content":"","date":1660608000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1660608000,"objectID":"11fba3710f90baa014e4e51f54f045f2","permalink":"https://mkegler.github.io/publication/cherep-2022/","publishdate":"2022-08-16T00:02:40.369174Z","relpermalink":"/publication/cherep-2022/","section":"publication","summary":"Flow is a mental state experienced during holistic involvement in a certain task, and it is a factor that promotes motivation, development, and performance. A reliable and objective estimation of the flow is essential for moving away from the traditional self-reporting subjective questionnaires, and for developing closed-loop human-computer interfaces. In this study, we recorded EEG and pupil dilation in a cohort of participants solving arithmetic problems. In particular, the EEG activity was acquired with a prototype of a commercial headset from Logitech with nine dry electrodes incorporated in a pair of over-ear headphones. The difficulty of the tasks was adapted to induce mental Boredom, Flow and Overload, corresponding to too easy, optimal and too challenging tasks, respectively. Results indicated statistically significant differences between all pairs of conditions for the pupil dilation, as well as for the EEG activity for the electrodes in the ear-pads. Furthermore, we built a predictive model that estimated the mental state of the user from their EEG data with 65% accuracy.","tags":null,"title":"Mental Flow Estimation Through Wearable EEG","type":"publication"},{"authors":["Mikolaj Kegler","Hugo Weissbart","Tobias Reichenbach"],"categories":null,"content":"","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656633600,"objectID":"5ee3e7cfc1fc854b19442d231d22cbd2","permalink":"https://mkegler.github.io/publication/kegler-2022/","publishdate":"2022-07-01T17:02:40.369174Z","relpermalink":"/publication/kegler-2022/","section":"publication","summary":"Spoken language comprehension requires rapid and continuous integration of information, from lower-level acoustic to higher-level linguistic features. Much of this processing occurs in the cerebral cortex. Its neural activity exhibits, for instance, correlates of predictive processing, emerging at delays of a few 100 ms. However, the auditory pathways are also characterized by extensive feedback loops from higher-level cortical areas to lower-level ones as well as to subcortical structures. Early neural activity can therefore be influenced by higher-level cognitive processes, but it remains unclear whether such feedback contributes to linguistic processing. Here, we investigated early speech-evoked neural activity that emerges at the fundamental frequency. We analyzed EEG recordings obtained when subjects listened to a story read by a single speaker. We identified a response tracking the speaker's fundamental frequency that occurred at a delay of 11 ms, while another response elicited by the high-frequency modulation of the envelope of higher harmonics exhibited a larger magnitude and longer latency of about 18 ms with an additional significant component at around 40 ms. Notably, while the earlier components of the response likely originate from the subcortical structures, the latter presumably involves contributions from cortical regions. Subsequently, we determined the magnitude of these early neural responses for each individual word in the story. We then quantified the context-independent frequency of each word and used a language model to compute context-dependent word surprisal and precision. The word surprisal represented how predictable a word is, given the previous context, and the word precision reflected the confidence about predicting the next word from the past context. We found that the word-level neural responses at the fundamental frequency were predominantly influenced by the acoustic features: the average fundamental frequency and its variability. Amongst the linguistic features, only context-independent word frequency showed a weak but significant modulation of the neural response to the high-frequency envelope modulation. Our results show that the early neural response at the fundamental frequency is already influenced by acoustic as well as linguistic information, suggesting top-down modulation of this neural response.","tags":null,"title":"The neural response at the fundamental frequency of speech is modulated by word-level acoustic and linguistic information","type":"publication"},{"authors":[],"categories":[],"content":"","date":1652659200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652659200,"objectID":"437f75ca5117402fed31e23aaa142c14","permalink":"https://mkegler.github.io/project/hybrid-byol-s/","publishdate":"2022-05-16T00:00:00Z","relpermalink":"/project/hybrid-byol-s/","section":"project","summary":"(Code) Hybrid BYOL speech representation learning","tags":["DL","SP"],"title":"Hybrid BYOL-S","type":"project"},{"authors":["Neil Scheidwasser-Clow","Mikolaj Kegler","Pierre Beckmann","Milos Cernak"],"categories":null,"content":"","date":1652659200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652659200,"objectID":"96f0acc3dcf4c91ae556dce5f7626ba3","permalink":"https://mkegler.github.io/publication/scheidwasser-clow-2022/","publishdate":"2022-05-16T00:02:40.369174Z","relpermalink":"/publication/scheidwasser-clow-2022/","section":"publication","summary":"Recent developments in speech emotion recognition (SER) often leverage deep neural networks (DNNs). Comparing and benchmarking different DNN models can often be tedious due to the use of different datasets and evaluation protocols. To facilitate the process, here, we present the Speech Emotion Recognition Adaptation Benchmark (SERAB), a framework for evaluating the performance and generalization capacity of different approaches for utterance-level SER. The benchmark is composed of nine datasets for SER in six languages. Since the datasets have different sizes and numbers of emotional classes, the proposed setup is particularly suitable for estimating the generalization capacity of pre-trained DNN-based feature extractors. We used the proposed framework to evaluate a selection of standard hand-crafted feature sets and state-of-the-art DNN representations. The results highlight that using only a subset of the data included in SERAB can result in biased evaluation, while compliance with the proposed protocol can circumvent this issue.","tags":null,"title":"SERAB: A multi-lingual benchmark for speech emotion recognition","type":"publication"},{"authors":["Mikolaj Kegler"],"categories":null,"content":"","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"356d13dd252c3afcdecd96919ac3e6d5","permalink":"https://mkegler.github.io/publication/kegler-thesis/","publishdate":"2022-05-01T00:00:00Z","relpermalink":"/publication/kegler-thesis/","section":"publication","summary":"Humans are highly skilled at the analysis of complex auditory scenes. In particular, the human auditory system is characterized by incredible robustness to noise and can nearly effortlessly isolate the voice of a specific talker from even the busiest of mixtures. However, neural mechanisms underlying these remarkable properties remain poorly understood. This is mainly due to the inherent complexity of speech signals and multi-stage, intricate processing performed in the human auditory system. Understanding these neural mechanisms underlying speech perception is of interest for clinical practice, brain-computer interfacing and automatic speech processing systems. In this thesis, we developed computational models characterizing neural speech processing across different stages of the human auditory pathways. In particular, we studied the active role of slow cortical oscillations in speech-in-noise comprehension through a spiking neural network model for encoding spoken sentences. The neural dynamics of the model during noisy speech encoding reflected speech comprehension of young, normal-hearing adults. The proposed theoretical model was validated by predicting the effects of non-invasive brain stimulation on speech comprehension in an experimental study involving a cohort of volunteers. Moreover, we developed a modelling framework for detecting the early, high-frequency neural response to the uninterrupted speech in non-invasive neural recordings. We applied the method to investigate top-down modulation of this response by the listener's selective attention and linguistic properties of different words from a spoken narrative. We found that in both cases, the detected responses of predominantly subcortical origin were significantly modulated, which supports the functional role of feedback, between higher- and lower levels stages of the auditory pathways, in speech perception. The proposed computational models shed light on some of the poorly understood neural mechanisms underlying speech perception. The developed methods can be readily employed in future studies involving a range of experimental paradigms beyond these considered in this thesis.","tags":null,"title":"Computational modelling of neural mechanisms underlying natural speech perception","type":"publication"},{"authors":["Anirudh Kulkarni","Mikolaj Kegler","Tobias Reichenbach"],"categories":null,"content":"","date":1632268800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632268800,"objectID":"7c2be7cf5d79fb516bcf4766a86fa429","permalink":"https://mkegler.github.io/publication/kulkarni-2021/","publishdate":"2021-09-22T19:02:40.369174Z","relpermalink":"/publication/kulkarni-2021/","section":"publication","summary":"Seeing a person talking can help to understand them, in particular in a noisy environment. However, how the brain integrates the visual information with the auditory signal to enhance speech comprehension remains poorly understood. Here we address this question in a computational model of a cortical microcircuit for speech processing. The model consists of an excitatory and an inhibitory neural population that together create oscillations in the theta frequency range. When simulated with speech, the theta rhythm becomes entrained to the onsets of syllables, such that the onsets can be inferred from the network activity. We investigate how well the obtained syllable parsing performs when different types of visual stimuli are added. In particular, we consider currents related to the rate of syllables as well as currents related to the mouth-opening area of the talking faces. We find that currents that target the excitatory neuronal population can influence speech comprehension, both boosting it or impeding it, depending on the temporal delay and on whether the currents are excitatory or inhibitory. In contrast, currents that act on the inhibitory neurons do not impact speech comprehension significantly. Our results suggest neural mechanisms for the integration of visual information with the acoustic information in speech and make experimentally-testable predictions.","tags":null,"title":"Effect of visual input on syllable parsing in a computational model of a neural microcircuit for speech processing","type":"publication"},{"authors":["Pierre Beckmann*","Mikolaj Kegler*","Milos Cernak"],"categories":null,"content":"","date":1628467200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1628467200,"objectID":"dbea739fc2f499c2210f7e5b581d436a","permalink":"https://mkegler.github.io/publication/beckmann-2021/","publishdate":"2021-08-09T17:02:40.369174Z","relpermalink":"/publication/beckmann-2021/","section":"publication","summary":"Recent breakthroughs in deep learning often rely on representation learning and knowledge transfer. In recent years, unsupervised and self-supervised techniques for learning speech representation were developed to foster automatic speech recognition. Up to date, most of these approaches are task-specific and designed for within-task transfer learning between different datasets or setups of a particular task. In turn, learning task-independent representation of speech and cross-task applications of transfer learning remain less common. Here, we introduce an encoder capturing word-level representations of speech for cross-task transfer learning. We demonstrate the application of the pre-trained encoder in four distinct speech and audio processing tasks: (i) speech enhancement, (ii) language identification, (iii) speech, noise, and music classification, and (iv) speaker identification. In each task, we compare the performance of our cross-task transfer learning approach to task-specific baselines. Our results show that the speech representation captured by the encoder through the pre-training is transferable across distinct speech processing tasks and datasets. Notably, even simple applications of our pre-trained encoder outperformed task-specific methods, or were comparable, depending on the task.","tags":null,"title":"Word-Level Embeddings for Cross-Task Transfer Learning in Speech Processing","type":"publication"},{"authors":["Mikolaj Kegler","Tobias Reichenbach"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"bc65892fd1ff18dd88794859e15711b4","permalink":"https://mkegler.github.io/publication/kegler-2021/","publishdate":"2021-01-01T00:00:01.369507Z","relpermalink":"/publication/kegler-2021/","section":"publication","summary":"Transcranial alternating current stimulation (tACS) can non-invasively modulate neuronal activity in the cerebral cortex, in particular at the frequency of the applied stimulation. Such modulation can matter for speech processing, since the latter involves the tracking of slow amplitude fluctuations in speech by cortical activity. tACS with a current signal that follows the envelope of a speech stimulus has indeed been found to influence the cortical tracking and to modulate the comprehension of the speech in background noise. However, how exactly tACS influences the speech-related cortical activity, and how it causes the observed effects on speech comprehension, remains poorly understood. A computational model for cortical speech processing in a biophysically plausible spiking neural network has recently been proposed. Here we extended the model to investigate the effects of different types of stimulation waveforms, similar to those previously applied in experimental studies, on the processing of speech in noise. We assessed in particular how well speech could be decoded from the neural network activity when paired with the exogenous stimulation. We found that, in the absence of current stimulation, the speech-in-noise decoding accuracy was comparable to the comprehension of speech in background noise of human listeners. We further found that current stimulation could alter the speech decoding accuracy by a few percent, comparable to the effects of tACS on speech-in-noise comprehension. Our simulations further allowed us to identify the parameters for the stimulation waveforms that yielded the largest enhancement of speech-in-noise encoding. Our model thereby provides insight into the potential neural mechanisms by which weak alternating current stimulation may influence speech comprehension and allows to screen a large range of stimulation waveforms for their effect on speech processing.","tags":null,"title":"Modelling the effects of transcranial alternating current stimulation on the neural encoding of speech in noise","type":"publication"},{"authors":[],"categories":[],"content":"","date":1607040000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607040000,"objectID":"7677bcd5713aafb1faba7264c42bd423","permalink":"https://mkegler.github.io/project/serab/","publishdate":"2020-12-04T00:00:00Z","relpermalink":"/project/serab/","section":"project","summary":"(Code) A multi-lingual benchmark for speech emotion recognition","tags":["DL","SP"],"title":"SERAB","type":"project"},{"authors":["Mikolaj Kegler*","Pierre Beckmann*","Milos Cernak"],"categories":null,"content":"","date":1603843200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603843200,"objectID":"60c68790ce8b931ef12dc2b586a30bd3","permalink":"https://mkegler.github.io/publication/kegler-2019/","publishdate":"2020-10-28T00:01:40.369507Z","relpermalink":"/publication/kegler-2019/","section":"publication","summary":"Transient loud intrusions, often occurring in noisy environments, can completely overpower speech signal and lead to an inevitable loss of information. While existing algorithms for noise suppression can yield impressive results, their efficacy remains limited for very low signal-to-noise ratios or when parts of the signal are missing. To address these limitations, here we propose an end-to-end framework for speech inpainting, the context-based retrieval of missing or severely distorted parts of time-frequency representation of speech. The framework is based on a convolutional U-Net trained via deep feature losses, obtained using speechVGG, a deep speech feature extractor pre-trained on an auxiliary word classification task. Our evaluation results demonstrate that the proposed framework can recover large portions of missing or distorted time-frequency representation of speech, up to 400 ms and 3.2 kHz in bandwidth. In particular, our approach provided a substantial increase in STOI \u0026 PESQ objective metrics of the initially corrupted speech samples. Notably, using deep feature losses to train the framework led to the best results, as compared to conventional approaches.","tags":null,"title":"Deep Speech Inpainting of Time-Frequency Masks","type":"publication"},{"authors":[],"categories":[],"content":"","date":1591228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591228800,"objectID":"eb93cb81cec9d25fcaa97fe9998d353d","permalink":"https://mkegler.github.io/project/speechtacs/","publishdate":"2020-06-04T00:00:00Z","relpermalink":"/project/speechtacs/","section":"project","summary":"(Code) Computational model for the effect of non-invasive brain stimulation on speech in noise processing.","tags":["Neuro","CM"],"title":"Modelling the effects of tACS on speech processing","type":"project"},{"authors":[],"categories":[],"content":"","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"7487384165091559ab0a8b61edecb4a4","permalink":"https://mkegler.github.io/project/pynsl/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/project/pynsl/","section":"project","summary":"(Code) Python port of the NSL toolbox used for auditory modelling.","tags":["Neuro","CM","SP"],"title":"pyNSL","type":"project"},{"authors":[],"categories":[],"content":"","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"1eae9888cddb9e94b58b5622183425e5","permalink":"https://mkegler.github.io/project/pyeeg/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/project/pyeeg/","section":"project","summary":"(Code) Custom set of tools for EEG processing and analysis.","tags":["Neuro"],"title":"sPyEEG","type":"project"},{"authors":["Frederique Vanheusden","Mikolaj Kegler","Katie Ireland","Constantina Georga","David Simpson","Tobias Reichenbach","Steven L Bell"],"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585699200,"objectID":"d46ee63795830b5e27fa421824ef2b85","permalink":"https://mkegler.github.io/publication/vanheusden-2020/","publishdate":"2020-06-10T19:04:40.368334Z","relpermalink":"/publication/vanheusden-2020/","section":"publication","summary":"**Background**: Cortical entrainment to speech correlates with speech intelligibility and attention to a speech stream in noisy environments. However, there is a lack of data on whether cortical entrainment can help in evaluating hearing aid fittings for subjects with mild to moderate hearing loss. One particular problem that may arise is that hearing aids may alter the speech stimulus during (pre-)processing steps, which might alter cortical entrainment to the speech. Here, the effect of hearing aid processing on cortical entrainment to running speech in hearing impaired subjects was investigated.\n**Methodology**: Seventeen native English-speaking subjects with mild-to-moderate hearing loss participated in the study. Hearing function and hearing aid fitting were evaluated using standard clinical procedures. Participants then listened to a 25-minute audiobook under aided and unaided conditions at 70 dB A sound pressure level (SPL) in quiet conditions. EEG data were collected using a 32-channel system. Cortical entrainment to speech was evaluated using decoders reconstructing the speech envelope from the EEG data. Null decoders, obtained from EEG and the time-reversed speech envelope, were used to assess the chance level reconstructions. Entrainment in the delta- (1-4 Hz) and theta- (4-8 Hz) band, as well as wideband (1-20 Hz) EEG data was investigated.\n**Results**: Significant cortical responses could be detected for all but one subject in all three frequency bands under both aided and unaided conditions. However, no significant differences could be found between the two conditions in the number of responses detected, nor in the strength of cortical entrainment. The results show that the relatively small change in speech input provided by the hearing aid was not sufficient to elicit a detectable change in cortical entrainment.\n**Conclusion**: For subjects with mild to moderate hearing loss, cortical entrainment to speech in quiet at an audible level is not affected by hearing aids. These results clear the pathway for exploring the potential to use cortical entrainment to running speech for evaluating hearing aid fitting at lower speech intensities (which could be inaudible when unaided), or using speech in noise conditions.","tags":null,"title":"Hearing aids do not alter cortical entrainment to speech at audible levels in mild-to-moderately hearing-impaired subjects","type":"publication"},{"authors":["Mahmoud Keshavarzi","Mikolaj Kegler","Shabnam Kadir","Tobias Reichenbach"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"07c629a756b88c8d646a1ac9fc76def7","permalink":"https://mkegler.github.io/publication/keshavarzi-2020/","publishdate":"2020-06-10T19:03:40.368831Z","relpermalink":"/publication/keshavarzi-2020/","section":"publication","summary":"Auditory cortical activity entrains to speech rhythms and has been proposed as a mechanism for online speech processing. In particular, neural activity in the theta frequency band (4–8 ​Hz) tracks the onset of syllables which may aid the parsing of a speech stream. Similarly, cortical activity in the delta band (1–4 ​Hz) entrains to the onset of words in natural speech and has been found to encode both syntactic as well as semantic information. Such neural entrainment to speech rhythms is not merely an epiphenomenon of other neural processes, but plays a functional role in speech processing: modulating the neural entrainment through transcranial alternating current stimulation influences the speech-related neural activity and modulates the comprehension of degraded speech. However, the distinct functional contributions of the delta- and of the theta-band entrainment to the modulation of speech comprehension have not yet been investigated. Here we use transcranial alternating current stimulation with waveforms derived from the speech envelope and filtered in the delta and theta frequency bands to alter cortical entrainment in both bands separately. We find that transcranial alternating current stimulation in the theta band but not in the delta band impacts speech comprehension. Moreover, we find that transcranial alternating current stimulation with the theta-band portion of the speech envelope can improve speech-in-noise comprehension beyond sham stimulation. Our results show a distinct contribution of the theta- but not of the delta-band stimulation to the modulation of speech comprehension. In addition, our findings open up a potential avenue of enhancing the comprehension of speech in noise.","tags":null,"title":"Transcranial alternating current stimulation in the theta band but not in the delta band modulates the comprehension of naturalistic speech in noise","type":"publication"},{"authors":[],"categories":[],"content":"","date":1571702400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571702400,"objectID":"5d0cd112565e7eeaa4d8cc114bbf3095","permalink":"https://mkegler.github.io/project/speechvgg/","publishdate":"2019-10-22T00:00:00Z","relpermalink":"/project/speechvgg/","section":"project","summary":"(Code) Transferable pre-trained feature extractor for speech processing.","tags":["DL","SP"],"title":"Speech-VGG","type":"project"},{"authors":[],"categories":[],"content":"","date":1571529600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571529600,"objectID":"f5bbfce29ec9d9c922574eddd810041c","permalink":"https://mkegler.github.io/project/inpainting/","publishdate":"2019-10-20T00:00:00Z","relpermalink":"/project/inpainting/","section":"project","summary":"(Demo) Algorithm for recovering missing or severely degraded parts of time-frequency representations of speech.","tags":["DL","SP"],"title":"Deep Speech Inpainting","type":"project"},{"authors":[],"categories":[],"content":"","date":1560556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560556800,"objectID":"3972a395411d406f0bf6884f73f20d4f","permalink":"https://mkegler.github.io/project/ctrf/","publishdate":"2019-06-15T00:00:00Z","relpermalink":"/project/ctrf/","section":"project","summary":"(Code) Complex TRFs for modelling auditory brainstem responses to continuous speech from full-cap EEG","tags":["Neuro"],"title":"Complex TRF","type":"project"},{"authors":[],"categories":[],"content":"","date":1560124800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560124800,"objectID":"893027adaa496a735c03f2fb71a3c82e","permalink":"https://mkegler.github.io/project/f0_waveform/","publishdate":"2019-06-10T00:00:00Z","relpermalink":"/project/f0_waveform/","section":"project","summary":"(Code) EMD-based algorithm for the extraction of F0 waveform from continuous speech. Maintained code. Original implementation by A.E. Forte.","tags":["Neuro","SP","CM"],"title":"Fundamental waveform extraction","type":"project"},{"authors":["Octave Etard*","Mikolaj Kegler*","Chananel Braiman","Antonio Elia Forte","Tobias Reichenbach"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"113a380792396632de6b35f57053e23b","permalink":"https://mkegler.github.io/publication/etard-2019/","publishdate":"2020-06-10T19:00:40.369742Z","relpermalink":"/publication/etard-2019/","section":"publication","summary":"Humans are highly skilled at analysing complex acoustic scenes. The segregation of different acoustic streams and the formation of corresponding neural representations is mostly attributed to the auditory cortex. Decoding of selective attention from neuroimaging has therefore focussed on cortical responses to sound. However, the auditory brainstem response to speech is modulated by selective attention as well, as recently shown through measuring the brainstem's response to running speech. Although the response of the auditory brainstem has a smaller magnitude than that of the auditory cortex, it occurs at much higher frequencies and therefore has a higher information rate. Here we develop statistical models for extracting the brainstem response from multi-channel scalp recordings and for analysing the attentional modulation according to the focus of attention. We demonstrate that the attentional modulation of the brainstem response to speech can be employed to decode the attentional focus of a listener from short measurements of 10 s or less in duration. The decoding remains accurate when obtained from three EEG channels only. We further show how out-of-the-box decoding that employs subject-independent models, as well as decoding that is independent of the specific attended speaker is capable of achieving similar accuracy. These results open up new avenues for investigating the neural mechanisms for selective attention in the brainstem and for developing efficient auditory brain-computer interfaces.","tags":["Auditory attention decoding","Complex auditory brainstem response","Natural speech"],"title":"Decoding of selective attention to continuous speech from the human auditory brainstem response","type":"publication"}]